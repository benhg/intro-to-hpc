{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3. Parallel Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Trivially Parallelizable Algorithms\n",
    "In order to properly make use of an HPC system or other parallel computing environment, you need to be able to make your code able to run in parallel. As we saw with our monte carlo pi example, even making the same algorithm run in parallel can make huge changes in terms of performance. If a job running on an HPC system has access to hundreds of cores, but is only written to run in serial, it will not make use of the computational resources it has available to it. In theory, if your task is completely parallelizable, then if you scale it up by a factor of _n_ cores, it will speed up _n_ times. That is to say, if you run a completely parallelizable job on 20 cores, it will run up to 20 times faster than on one core. A graph of this is below: ![Theoretical maximum speedup](https://cdn.comsol.com/wordpress/2014/03/Graph-depicting-how-the-size-of-the-job-increases-with-the-number-of-available-processes.png)\n",
    "This graphic represents the theoretical maximum speedup of various types of HPC jobs, where the x-axis represents the number of cores the job is run on, the y axis represents how much faster the job runs (i.e. if the y-value is 20, the job is 20 times faster than single core), and the value of _phi_ represents the fraction of the job that can be parallelized. The mathematical principle this is based on is called [The Gustafson-Barsis Law](https://en.wikipedia.org/wiki/Gustafson%27s_law)\n",
    "\n",
    "The class of algorithms for which the value of _phi_ is 1, that is to say the class of algorithms that scale perfectly, is known as the \"trivially parallelizable\" class. This means that you can expect linear performance scaling as you increase number of cores linearly. These algorithms are, rather unsurprisingly, very easy to make parallel. Because of this, people using HPC systems often try to reduce their workloads to different \"building blocks\" made up of trivially parallelizable algorithms. A Trivially parallelizable algorithm is defined by task-independence. That is to say, if you break up an algorithm into tasks, in order for that algorithm to be trivially parallelizable, each task must not depend on the output of any other task. The image below represents a trivially parallelizable algorithm:\n",
    "![trivially parallel](http://matthewrocklin.com/slides/images/embarrassing-process.png)\n",
    "Each set of input data goes into a process, and comes out changed. None of them depend on what is happening in other simultaneous processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1 - Generation of Data\n",
    "As an example of a trivially parallelizable algorithm, we are going to generate lots of numbers in serial and in parallel. This will also teach you a very practical skill with python's `multiprocessing` library: how to \"unroll\" a trivially parallelizable loop into processes that can run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "CPU times: user 92.2 ms, sys: 71.5 ms, total: 164 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generating numbers in serial\n",
    "\n",
    "with open(\"/home/users/glick/intro-to-hpc/data/datagen.out\" ,\"w\") as file:\n",
    "    # Printing numbers to file\n",
    "    for i in range(1,100001):\n",
    "        file.write(str(i)+'\\n')\n",
    "        \n",
    "with open(\"/home/users/glick/intro-to-hpc/data/datagen.out\") as file:\n",
    "    # Reading numbers from file. Don't print them all out because there're too many\n",
    "    print(len(file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.1 ms, sys: 11.6 ms, total: 98.7 ms\n",
      "Wall time: 62.6 ms\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Generating numbers in parallel\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "file = open(\"/home/users/glick/intro-to-hpc/data/datagenparallel.out\" ,\"w\")\n",
    "\n",
    "# We simply replace the for loop with a function...\n",
    "def process_single(i):\n",
    "    file.write(str(i)+'\\n')\n",
    "\n",
    "# Create a multiprocessing Pool\n",
    "pool = Pool(32)\n",
    "\n",
    "#... And then map our inputs to it as follows:\n",
    "tasks = [pool.apply_async(process_single, (j,)) for j in range(1,1000)]\n",
    "\n",
    "# We use the `get()` function of the last submitted task to time how long it takes to run the process\n",
    "%time tasks[-1].get()\n",
    "\n",
    "#Clean up the files\n",
    "time.sleep(3)\n",
    "file.close()\n",
    "\n",
    "with open(\"/home/users/glick/intro-to-hpc/data/datagenparallel.out\") as file2:\n",
    "    # Reading numbers from file. Don't print them all out because there're too many\n",
    "    print(len(file2.readlines()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that writing the code in parallel is not much more complex than writing it in serial. In this case, the task is carried out so quickly in serial that it is not really worth it to parallelize, but there are a multitude of real-life scenarios where a workflow, or at least part of a workflow is trivially parallelizable and it makes sense to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Monte Carlo Simulations\n",
    "Remember in the last section how I made a fuss about how, even though it seems really unlikely that real world applications would ever be even close to trivially parallelizable, it's a goal that many scientists and other HPC users aim for? Well, one of the tools these people commonly use to move that direction is the _monte carlo simulation_. We've already used monte carlo simulations to compute a numerical value of pi experimentally, but we haven't really strictly defined just what exactly a monte carlo algorithm _is_, so let's do that now. A _monte carlo algorithm_ is an algorithm that attempts to produce an answer to a question by simulating many possible outcomes. For example, our monte carlo pi simulates throwing a dart at a dartboard over and over, and then uses the underlying geometry of the situation to extract a value from those darts' randomly selected locations. A monte carlo algorithm is often trivially parallelizable, because random numbers, by definintion, can always be generated independently, that is with no input from each other. For this reason, many HPC users attempt to use the monte carlo method when attempting to solve problems with \"real\" values (i.e. numerically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2 - Monte Carlo Frog Hop Simulation\n",
    "To illustrate the scalability of monte carlo algorithms, we're going to use an example problem which my professor Jeff Ely uses to test the mettle of CS 1 students. Imagine a frog who is sitting on an infinite flat plane. The frog always makes jumps of length 1 unit, in a randomly selected direction. What is the probablility that the frog will be within 1 unit of its original location after _n_ jumps?\n",
    "![frog-setup]()\n",
    "You may realize that we can solve this problem with no programming at all, just math In this case, this is true, and we can use this fact to help us check our work. Often, especially in the fields of physics and differential equations, there are problems which cannot be solved other than numerically. We will solve this problem first in serial and then we will parallelize our solution later.### Example 3.2 - Monte Carlo Frog Hop Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the frog jumps 0 times, it will land in the original circle          approximately 100000 times, representing a success rate of 1.0\n",
      "If the frog jumps 1 times, it will land in the original circle          approximately 100000 times, representing a success rate of 1.0\n",
      "If the frog jumps 2 times, it will land in the original circle          approximately 33255 times, representing a success rate of 0.33255\n",
      "If the frog jumps 3 times, it will land in the original circle          approximately 25074 times, representing a success rate of 0.25074\n",
      "If the frog jumps 4 times, it will land in the original circle          approximately 19770 times, representing a success rate of 0.1977\n",
      "If the frog jumps 5 times, it will land in the original circle          approximately 16829 times, representing a success rate of 0.16829\n",
      "If the frog jumps 6 times, it will land in the original circle          approximately 14117 times, representing a success rate of 0.14117\n",
      "If the frog jumps 7 times, it will land in the original circle          approximately 12508 times, representing a success rate of 0.12508\n",
      "If the frog jumps 8 times, it will land in the original circle          approximately 11053 times, representing a success rate of 0.11053\n",
      "If the frog jumps 9 times, it will land in the original circle          approximately 9964 times, representing a success rate of 0.09964\n",
      "If the frog jumps 10 times, it will land in the original circle          approximately 9139 times, representing a success rate of 0.09139\n",
      "If the frog jumps 11 times, it will land in the original circle          approximately 8338 times, representing a success rate of 0.08338\n",
      "If the frog jumps 12 times, it will land in the original circle          approximately 7759 times, representing a success rate of 0.07759\n",
      "If the frog jumps 13 times, it will land in the original circle          approximately 7188 times, representing a success rate of 0.07188\n",
      "If the frog jumps 14 times, it will land in the original circle          approximately 6692 times, representing a success rate of 0.06692\n",
      "If the frog jumps 15 times, it will land in the original circle          approximately 6270 times, representing a success rate of 0.0627\n",
      "If the frog jumps 16 times, it will land in the original circle          approximately 5736 times, representing a success rate of 0.05736\n",
      "If the frog jumps 17 times, it will land in the original circle          approximately 5563 times, representing a success rate of 0.05563\n",
      "If the frog jumps 18 times, it will land in the original circle          approximately 5291 times, representing a success rate of 0.05291\n",
      "If the frog jumps 19 times, it will land in the original circle          approximately 4873 times, representing a success rate of 0.04873\n",
      "CPU times: user 30.2 s, sys: 5.49 s, total: 35.6 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Serial frog solution\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def monteCarlo(numJumps):\n",
    "    numTries=100000\n",
    "    lenJump=1\n",
    "    numSuccesses=0\n",
    "\n",
    "    #randomly test 1 million times and see what happens\n",
    "    for j in range(0,numTries,1):\n",
    "        #vector representing distance of 0 from origin\n",
    "        frogPosition=[0.0,0.0]\n",
    "        for i in range(0,numJumps,1):\n",
    "            #generate a random angle\n",
    "            theta=random.uniform(0,2*math.pi)\n",
    "            #add to x and y components of frog position vectors\n",
    "            frogPosition[0]+=lenJump*math.cos(theta)\n",
    "            frogPosition[1]+=lenJump*math.sin(theta)\n",
    "        #compute magnitude of final frog position vector\n",
    "        frogMagnitude=((frogPosition[0]**2+frogPosition[1]**2)**0.5)\n",
    "\n",
    "        #check if frog landed where we wanted it to\n",
    "        if frogMagnitude<=1.0:\n",
    "            #keep track of successes\n",
    "            numSuccesses+=float(1)\n",
    "    \n",
    "    #compute success rate\n",
    "    successRate=float(numSuccesses/numTries)    \n",
    "    print(\"If the frog jumps %s times, it will land in the original circle\\\n",
    "approximately %s times, representing a success rate of %s\"%(numJumps,int(numSuccesses),successRate))\n",
    "\n",
    "    \n",
    "for i in range(20):\n",
    "    monteCarlo(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the frog jumps 2 times, it will land in the original circleapproximately 33426 times, representing a success rate of 0.33426\n",
      "If the frog jumps 3 times, it will land in the original circleapproximately 24993 times, representing a success rate of 0.24993\n",
      "If the frog jumps 4 times, it will land in the original circleapproximately 19899 times, representing a success rate of 0.19899\n",
      "If the frog jumps 5 times, it will land in the original circleapproximately 16659 times, representing a success rate of 0.16659\n",
      "If the frog jumps 6 times, it will land in the original circleapproximately 14228 times, representing a success rate of 0.14228\n",
      "If the frog jumps 7 times, it will land in the original circleapproximately 12543 times, representing a success rate of 0.12543\n",
      "If the frog jumps 8 times, it will land in the original circleapproximately 11196 times, representing a success rate of 0.11196\n",
      "If the frog jumps 9 times, it will land in the original circleapproximately 9989 times, representing a success rate of 0.09989\n",
      "If the frog jumps 10 times, it will land in the original circleapproximately 9049 times, representing a success rate of 0.09049\n",
      "If the frog jumps 12 times, it will land in the original circleapproximately 7630 times, representing a success rate of 0.0763\n",
      "If the frog jumps 11 times, it will land in the original circleapproximately 8324 times, representing a success rate of 0.08324\n",
      "If the frog jumps 13 times, it will land in the original circleapproximately 7152 times, representing a success rate of 0.07152\n",
      "If the frog jumps 14 times, it will land in the original circleapproximately 6576 times, representing a success rate of 0.06576\n",
      "If the frog jumps 15 times, it will land in the original circleapproximately 6141 times, representing a success rate of 0.06141\n",
      "If the frog jumps 16 times, it will land in the original circleapproximately 5853 times, representing a success rate of 0.05853\n",
      "If the frog jumps 17 times, it will land in the original circleapproximately 5454 times, representing a success rate of 0.05454\n",
      "If the frog jumps 18 times, it will land in the original circleapproximately 5180 times, representing a success rate of 0.0518\n",
      "If the frog jumps 19 times, it will land in the original circleapproximately 5038 times, representing a success rate of 0.05038\n",
      "CPU times: user 229 ms, sys: 1.42 s, total: 1.65 s\n",
      "Wall time: 3.12 s\n",
      "If the frog jumps 1 times, it will land in the original circleapproximately 100000 times, representing a success rate of 1.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallel frog solution\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# We parallelize the same way here, by turning for loops into functions and mapping to them\n",
    "\n",
    "def outer_loop(numJumps):\n",
    "    numTries=100000\n",
    "    lenJump=1\n",
    "    numSuccesses=0\n",
    "    #randomly test 1 million times and see what happens\n",
    "    for j in range(numTries):\n",
    "        #vector representing distance of 0 from origin\n",
    "        frogPosition=[0.0,0.0]\n",
    "        for i in range(0,numJumps,1):\n",
    "            #generate a random angle\n",
    "            theta=random.uniform(0,2*math.pi)\n",
    "            #add to x and y components of frog position vectors\n",
    "            frogPosition[0]+=lenJump*math.cos(theta)\n",
    "            frogPosition[1]+=lenJump*math.sin(theta)\n",
    "        #compute magnitude of final frog position vector\n",
    "        frogMagnitude=((frogPosition[0]**2+frogPosition[1]**2)**0.5)\n",
    "\n",
    "        #check if frog landed where we wanted it to\n",
    "        if frogMagnitude<=1.0:\n",
    "            #keep track of successes\n",
    "            numSuccesses+=float(1)\n",
    "    \n",
    "    #compute success rate\n",
    "    successRate=float(numSuccesses/numTries)    \n",
    "    print(\"If the frog jumps %s times, it will land in the original circle\\\n",
    "approximately %s times, representing a success rate of %s\"%(numJumps,int(numSuccesses),successRate))\n",
    "\n",
    "    \n",
    "pool = Pool(32)\n",
    "tasks = [pool.apply_async(outer_loop, (j,)) for j in range(1,20)]\n",
    "tasks[-1].get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the parallel version is many times faster than the serial version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Single Instruction, Multiple Data\n",
    "A wide variety of different HPC workflows include some slight variation on what's called a _Single Instruction, Multiple Data_ algorithm. _Single Instruction, Multiple Data_, or SIMD algorithms are considered \"nice\" algorithms in that it is very easy to write scalable code for them. This code often behaves as well as trivially parallelizable algorithms, or with specialized hardware accelerators (like General Purpose GPUs), even better. HPC accelerators and heterogeneous computing is an extremely rich and wildly fascinating topic, and I highly recommend you check it out in the future, but it's a bit out of the scope of this course, so don't worry too much about GPGPUs or the like.\n",
    "That detour aside, The image below represents a SIMD process:\n",
    "![simd-architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/SIMD.svg/500px-SIMD.svg.png)\n",
    "\n",
    "In this example, the \"instruction pool\" provides each of the processing units, labeled \"PU\", the same instruction, and arbitrary data from the \"data\" pool gets fed into any of the processing units as they become available\n",
    "\n",
    "SIMD algorithms are certainly something you should look for when you're trying to speed up your code. They are easy to parallelize, and many different types of problems can be reduced to them. In addition, they can often be parallelized extremely effectively, because most modern processors have hardware specifically designed to perform SIMD instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3 - Vector Addition\n",
    "A common use case of parallelism in HPC is for performing vector math. Vector arithmetic comes up in all sorts of science, engineering, and other HPC applications. Many different pieces of science and math can be reduced, with some clever approximation, to vector math. In this example, we will take the sums of large vectors in serial and in parallel. Vector addition is an easily parallelizable task, because the way vector sums are computed is by taking pairwise sums of each corresponding vector subscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Vector addition in serial\n",
    "\n",
    "vec_a = [3]*1200000000\n",
    "vec_b = [4]*1200000000\n",
    "vec_c = [0]*1200000000\n",
    "\n",
    "for i in range(len(vec_a)):\n",
    "    vec_c[i] = vec_a[i] + vec_b[i]\n",
    "\n",
    "print(len(vec_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Vector addition in parallel\n",
    "\n",
    "vec_a = [3]*120000000\n",
    "vec_b = [4]*120000000\n",
    "vec_c = [0]*120000000\n",
    "\n",
    "def add_single(i):\n",
    "    vec_c[i] = vec_a[i] + vec_b[i]\n",
    "\n",
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(32)\n",
    "    \n",
    "pool.map(add_single, range(len(vec_a)))\n",
    "\n",
    "print(len(vec_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Multiple Instruction, Multiple Data\n",
    "SIMD algorithms are extremely performant, scalable, and easy to work with, but sometimes, you have a data pipeline where different things need to happen depending on what the data looks like. This scenario is often referred to as _Multiple Instruction, Multiple Data_, or MIMD. MIMD algorithms are much more flexible than SIMD ones, for obvious reasons. Every modern multicore computer can perform as a MIMD computer, to varying efficiencies. As of 2018, well over 95% of the TOP500 supercomputers use MIMD architectures as the basis for their computation. Of course, processors designed for MIMD computation can do SIMD computation as well, but they also have the ability to apply different kinds of logic based on data values. This ability makes MIMD extremely flexible for many diverse types of workloads. That detour aside, The image below represents a SIMD process:\n",
    "![mimd-architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/MIMD.svg/500px-SIMD.svg.png)\n",
    "In this image, multiple different types of data can come out of the \"data pool\" and are provided to the \"processing units\" along with various types of instructions from the \"instruction pool.\" Those instructions are not necessarily the same as each other, and can depend on the value of the data, randomization, or any other arbitrary logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4 - Asynchronous Branching Logic\n",
    "In this example, we are going to first, generate random numbers in parallel, and then, put them into separate files based on whether they are even or odd, and then finally, sort the files and print them out. We are going to use `multiprocessing` to perform each step in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '4', '4', '6', '6', '8', '10', '10', '10', '14', '16', '16', '20', '20', '28', '28', '32', '32', '32', '34', '34', '36', '40', '50', '50', '54', '56', '56', '56', '58', '64', '64', '64', '64', '68', '68', '70', '70', '76', '78', '80', '82', '84', '92', '96', '96', '98']\n",
      "\n",
      "\n",
      "['1', '3', '7', '7', '9', '11', '11', '11', '11', '13', '13', '23', '27', '27', '31', '31', '33', '39', '39', '41', '43', '47', '47', '47', '49', '49', '55', '57', '61', '65', '65', '67', '67', '69', '71', '73', '75', '75', '75', '75', '77', '81', '81', '81', '85', '87', '89', '89', '89', '93', '97', '97', '99']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"/home/users/glick/intro-to-hpc/data\" \n",
    "\n",
    "if os.path.isfile(path+\"/evenodd.out\"):\n",
    "    os.remove(path+\"/evenodd.out\")\n",
    "    \n",
    "if os.path.isfile(path+\"/even.out\"):\n",
    "    os.remove(path+\"/even.out\")\n",
    "\n",
    "if os.path.isfile(path+\"/odd.out\"):\n",
    "    os.remove(path+\"/odd.out\")\n",
    "\n",
    "# Write random numbers to file\n",
    "def gen_num(i):\n",
    "    file = open(path+\"/evenodd.out\", \"a\")\n",
    "    file.write(str(random.randint(0,100))+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "pool = multiprocessing.Pool(32)\n",
    "pool.map(gen_num, range(100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Put evens and odds in separate files\n",
    "def bin_out(item):\n",
    "    \n",
    "    if (int(item) % 2) == 0:\n",
    "        file2 = open(path+\"/even.out\", \"a\")\n",
    "        file2.write(str(item)+'\\n')\n",
    "        file2.close()\n",
    "    else:\n",
    "        file3 = open(path+\"/odd.out\", \"a\")\n",
    "        file3.write(str(item)+'\\n')\n",
    "        file3.close()\n",
    "\n",
    "# Reopen the file in read mode\n",
    "data = open(path+\"/evenodd.out\", \"r\").readlines()\n",
    "pool = multiprocessing.Pool(32)\n",
    "pool.map(bin_out, data)\n",
    "\n",
    "# sort the files\n",
    "def keyfunc(line):\n",
    "    try:\n",
    "        return int(line)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "#Sort the files in place\n",
    "for file in [path+\"/even.out\", path+\"/odd.out\"]:\n",
    "    with open(file) as fin:\n",
    "        content = sorted(fin, key=keyfunc)\n",
    "\n",
    "    with open(file, \"w\") as fout:\n",
    "        fout.writelines(content)\n",
    "\n",
    "# Show that it's ordered\n",
    "for file in [path+\"/even.out\", path+\"/odd.out\"]:\n",
    "    with open(file) as fin:\n",
    "        print([ x.strip() for x in fin.readlines() if x is not \"\\n\" ])\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Concurrency vs. Parallelism\n",
    "You may have heard the terms _concurrency_ and _parallelism_ before. They are often confused, and this makes sense because they both relate to running more than one process at a time. Concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Another reason they often get conflated is that they are often used together. \n",
    "\n",
    "A purely SIMD algorithm is a parallel environment, while any kind of MIMD algorithm includes at least some concurrency. Another time when concurrency is useful is when the workload you are trying to run involves interprocess communication, waiting for hard disk access, or waiting for network access. The reason for this is that it takes a lot of time to access the network, and a clever programmer will allow another process to use the spare CPU cycles that would be otherwise spent waiting. Hopefully, you can see how there is potential for overlap of parallelism and concurrency. \n",
    "\n",
    "Imagine a scenario where you want to generate files by connecting to an external service (maybe you're asking the Google maps API where something is), and then you want to perform some computation on those results. A purely parallel approach would be to make all of the Google API requests (all in parallel), wait for them all to finish, and then do all of the computation on each response, again in parallel. A better approach, one that uses concurrency, would be to make all of the API requests, and while some of the slower requests are waiting for their responses, allow the faster requests to begin their computation.\n",
    "\n",
    "The following image describes parallelism and concurrency: ![parallelism and concurrency](https://pbs.twimg.com/media/DSFCqf2U8AAjgqI.jpg)\n",
    "\n",
    "Using this image as a reference, we can imagine a scenario where there are multiple CPUs, as in the bottom image, and there are multiple queues per CPU, as in the top image. This would be a combination of parallelism and concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.5a - Network Interaction and IO-Bound Tasks\n",
    "As mentioned in the previous section, many of the tasks that lend themselves to concurrent execution are tasks with network or filesystem IO. This is because the amount of time it takes to connect to the external internet, or even the time it takes to connect to the local filesystem is much longer than the CPU's instruction cycle. This causes long periods of time (up to many seconds or minutes, depending on the task), where the CPU is just waiting for things to happen. Because of this, you are able to run many of these tasks all at the same time, because some can process while others wait, and vice-versa. In situations like this, you can run many more of these tasks than you have cores on your computer and expect reasonable performance speedups.\n",
    "\n",
    "In this example, we will create one sleep job and demonstrate that we can run sleep jobs concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    " \n",
    "NUM_WORKERS = 32\n",
    " \n",
    "def only_sleep():\n",
    "    \"\"\" Do nothing, wait for a timer to expire \"\"\"\n",
    "    print(\"PID: %s, Process Name: %s, Thread Name: %s\" % (\n",
    "        os.getpid(),\n",
    "        multiprocessing.current_process().name,\n",
    "        threading.current_thread().name)\n",
    "    )\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-34\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-35\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-36\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-37\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-38\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-39\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-40\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-41\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-42\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-43\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-44\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-45\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-46\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-47\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-48\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-49\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-50\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-51\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-52\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-53\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-54\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-55\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-56\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-57\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-58\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-59\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-60\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-61\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-62\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-63\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-64\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-65\n",
      "Threads time= 1.024193286895752 seconds\n"
     ]
    }
   ],
   "source": [
    "## Run tasks serially\n",
    "\n",
    "serial_run = False # Change to True if you want the serial version to run\n",
    "                   # It will take about 30 seconds\n",
    "\n",
    "if serial_run:\n",
    "    start_time = time.time()\n",
    "    for _ in range(NUM_WORKERS):\n",
    "        only_sleep()\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Serial time= {}\".format(end_time - start_time))\n",
    " \n",
    "# Run tasks using threads\n",
    "# Note that only one thread can run at a time, so this really is concurrency \n",
    "start_time = time.time()\n",
    "threads = [threading.Thread(target=only_sleep) for _ in range(NUM_WORKERS)]\n",
    "[thread.start() for thread in threads]\n",
    "[thread.join() for thread in threads]\n",
    "end_time = time.time()\n",
    " \n",
    "print(\"Threads time= {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.5b - Number Crunching and CPU-Bound Tasks\n",
    "Algorithms which can be run concurrently to great effect are often nice to work with. This is because they are, by definition, easy to speed up without too many resources. I like to refer to another class of easy to speed up tasks as _number crunching_. Tasks that fit into this category must a) have little or no input data, b) produce easy to handle amounts of output data, and c) use up at least an entire compute core for most of the time it needs to be running. Because of these requirements, Number crunching tasks are easy to speed up through parallelization, by adding more cores to a pool. They are slightly harder to speed up than tasks from the previous example, because they require additional hardware for more speedup, but they are still easy to speed up overall.\n",
    "\n",
    "In this example, we will create a number crunching task and show that it benefits from being run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number crunching workload\n",
    "\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "def crunch_numbers():\n",
    "    \"\"\" Do some computations \"\"\"\n",
    "    print(\"PID: %s, Process Name: %s, Thread Name: %s\" % (\n",
    "        os.getpid(),\n",
    "        multiprocessing.current_process().name,\n",
    "        threading.current_thread().name)\n",
    "    )\n",
    "    x = 0\n",
    "    while x < 10000000:\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-98\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-99\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-100\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-101\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-102\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-103\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-104\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-105\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-106\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-107\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-108\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-109\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-110\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-111\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-112\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-113\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-114\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-115\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-116\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-117\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-118\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-119\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-120\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-121\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-122\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-123\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-124\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-125\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-126\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-127\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-128\n",
      "PID: 26074, Process Name: MainProcess, Thread Name: Thread-129\n",
      "Threads time= 32.35811901092529 seconds\n",
      "PID: 11169, Process Name: Process-417, Thread Name: MainThread\n",
      "PID: 11170, Process Name: Process-418, Thread Name: MainThread\n",
      "PID: 11173, Process Name: Process-419, Thread Name: MainThread\n",
      "PID: 11178, Process Name: Process-420, Thread Name: MainThread\n",
      "PID: 11182, Process Name: Process-422, Thread Name: MainThread\n",
      "PID: 11179, Process Name: Process-421, Thread Name: MainThread\n",
      "PID: 11183, Process Name: Process-423, Thread Name: MainThread\n",
      "PID: 11188, Process Name: Process-424, Thread Name: MainThread\n",
      "PID: 11193, Process Name: Process-425, Thread Name: MainThread\n",
      "PID: 11194, Process Name: Process-426, Thread Name: MainThread\n",
      "PID: 11195, Process Name: Process-427, Thread Name: MainThread\n",
      "PID: 11196, Process Name: Process-428, Thread Name: MainThread\n",
      "PID: 11203, Process Name: Process-429, Thread Name: MainThread\n",
      "PID: 11208, Process Name: Process-430, Thread Name: MainThread\n",
      "PID: 11211, Process Name: Process-431, Thread Name: MainThread\n",
      "PID: 11214, Process Name: Process-432, Thread Name: MainThread\n",
      "PID: 11217, Process Name: Process-433, Thread Name: MainThread\n",
      "PID: 11218, Process Name: Process-434, Thread Name: MainThread\n",
      "PID: 11219, Process Name: Process-435, Thread Name: MainThread\n",
      "PID: 11227, Process Name: Process-436, Thread Name: MainThread\n",
      "PID: 11230, Process Name: Process-437, Thread Name: MainThread\n",
      "PID: 11241, Process Name: Process-438, Thread Name: MainThread\n",
      "PID: 11244, Process Name: Process-439, Thread Name: MainThread\n",
      "PID: 11245, Process Name: Process-440, Thread Name: MainThread\n",
      "PID: 11248, Process Name: Process-441, Thread Name: MainThread\n",
      "PID: 11253, Process Name: Process-442, Thread Name: MainThread\n",
      "PID: 11256, Process Name: Process-443, Thread Name: MainThread\n",
      "PID: 11259, Process Name: Process-444, Thread Name: MainThread\n",
      "PID: 11262, Process Name: Process-445, Thread Name: MainThread\n",
      "PID: 11266, Process Name: Process-447, Thread Name: MainThread\n",
      "PID: 11265, Process Name: Process-446, Thread Name: MainThread\n",
      "PID: 11269, Process Name: Process-448, Thread Name: MainThread\n",
      "Parallel time = 1.7069594860076904 seconds\n"
     ]
    }
   ],
   "source": [
    "thread_run = False # Change to True if you want the threads version to run\n",
    "                   # It will take about 30 seconds\n",
    "\n",
    "# This is here to illustrate that this task is NOT concurrent. \n",
    "# It will scale ONLY with increased parallelism\n",
    "if thread_run:\n",
    "    start_time = time.time()\n",
    "    threads = [threading.Thread(target=crunch_numbers) for _ in range(NUM_WORKERS)]\n",
    "    [thread.start() for thread in threads]\n",
    "    [thread.join() for thread in threads]\n",
    "    end_time = time.time()\n",
    "    print(\"Threads time= {} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Using almost the same workload from above, we will parallelize the crunch_numbers function\n",
    "\n",
    "start_time = time.time()\n",
    "processes = [multiprocessing.Process(target=crunch_numbers) for _ in range(NUM_WORKERS)]\n",
    "[process.start() for process in processes]\n",
    "[process.join() for process in processes]\n",
    "end_time = time.time()\n",
    " \n",
    "print(\"Parallel time = {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Dataflows\n",
    "At this point, you have learned about a whole bunch of different kinds of parallel, concurrent, and otherwise high-performance algorithms, and you should have a basic understanding of how to take regular, boring, slow serial code and use these tools to make it faster and more exciting. That's a great start as far as your parallel algorithms knowledge goes and it's fairly applicable to real world workloads that you may see, but I would be a bad teacher if I didn't spend at least a little time with _dataflows_.\n",
    "\n",
    "A _dataflow_ is a set of different algorithms, which may or may not be parallel, that all fit together in some way, with the output of some parts leading to the input of other parts. Most real world HPC applications are not just simple SIMD or MIMD blocks, but are instead a number of different pieces of code, all of which produce data based on other applications or input data. Each of those individual pieces of code may be SIMD or concurrent, or trivially parallel, or monte carlo, or any other kind of code, but the important part is understanding how they all work together. The dataflow is an incredibly powerful way of \"gluing\" many programs together into one program that carries out exactly what you want it to. The image below is a representation of a dataflow: ![basic dataflow](http://www.digitaleng.news/de/wp-content/uploads/2016/10/HPC-Workflow.jpg)\n",
    "In this case, data is first generated, then the quality of that data is assured, and then the data gets computed on, creating some output, and finally, that data gets visualized and consolidated to a human-readable format. This is a fairly common type of dataflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.6 - `sleep_fail` dataflow\n",
    "The dataflow presented in the previous section is a relatively typical dataflow, but it's also farily complex. To give you a more gentle introduction to working with dataflows, we are going to design a dataflow called `sleep_fail`. This dataflow consists of a data generation layer, in which we define a function which sleeps for _n_ (user input) seconds before failing with a probability _p_ (also user input). Then, it writes whether it failed or succeeded to a file. Then, we have a data consolidation layer, which reads that file and prints out how many times it ran, succeeded, and failed. The resulting workflow looks a bit like this:\n",
    "```\n",
    "    sleep1  sleep2 ...sleepN\n",
    "      |       |        |\n",
    "      V       V        v\n",
    "       \\      |       /\n",
    "        \\     |      /\n",
    "          sleep_Final\n",
    "```\n",
    "Note that this dataflow is serial. This is intended so that we can introduce you to the concept of a dataflow without worrying about parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define sleep_fail function\n",
    "def sleep_fail(n, p, path):\n",
    "    time.sleep(n)\n",
    "    if random.random() < p:\n",
    "        file = open(path, \"a\")\n",
    "        file.write(\"Exception\\n\")\n",
    "        file.close()\n",
    "        raise Exception\n",
    "    else:\n",
    "        file = open(path, \"a\")\n",
    "        file.write(\"Success\\n\")\n",
    "        file.close()\n",
    "        \n",
    "# Define summarize_sleeps\n",
    "def summarize_sleeps(path):\n",
    "    file = open(path, \"r\")\n",
    "    succeed = 0\n",
    "    exceptio = 0\n",
    "    for line in file.readlines():\n",
    "        if \"Exception\" in line:\n",
    "            exceptio +=1\n",
    "        elif \"Success\" in line:\n",
    "            succeed +=1\n",
    "    print(\"Failed {} Times\".format(exceptio))\n",
    "    print(\"Passed {} Times\".format(succeed))\n",
    "    print(\"Ran {} Times\".format(succeed + exceptio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 191 Times\n",
      "Passed 809 Times\n",
      "Ran 1000 Times\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/home/users/glick/intro-to-hpc/data/sleep_fail.out\"\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    os.remove(path)\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        sleep_fail(0, 0.2, path)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "summarize_sleeps(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Write a dataflow that estimates the value of _e_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
